# src/pdf_ingest.py
import os, re, json, glob, logging
from pathlib import Path
from tqdm import tqdm

logging.getLogger("pdfminer").setLevel(logging.ERROR)
logging.getLogger("pdfplumber").setLevel(logging.ERROR)

RAW_DIR = "data/raw_pdfs"
PARSED_DIR = "data/parsed"
Path(PARSED_DIR).mkdir(parents=True, exist_ok=True)

# ---- æ–°çš„å‹å·è¯†åˆ«å‡½æ•°ï¼šå…ˆçœ‹æ–‡ä»¶åï¼Œå†çœ‹æ–‡æœ¬ ----
def detect_model_from_string(s: str) -> str:
    # ç»Ÿä¸€å¤§å°å†™ & å»ç©ºæ ¼/è¿å­—ç¬¦ï¼Œä¾¿äºåŒ¹é…
    s0 = s.replace("-", " ").replace("_", " ").strip()

    # å…ˆåŒ¹é…ä¸å«æ•°å­—çš„ï¼šZ f / Z fc
    m = re.search(r"\bZ\s*f(c)?\b", s0, re.I)
    if m:
        return "ZF" if not m.group(1) else "ZFC"

    # å†åŒ¹é…å¸¸è§ï¼šZ 5/6/7 + II/III å¯é€‰ï¼›ä¹Ÿå…è®¸è¿å†™ Z6II / Z 6 II
    m = re.search(r"\bZ\s?([0-9]{1,2})(\s?I{2,3})?\b", s0, re.I)
    if m:
        num = m.group(1)
        roman = (m.group(2) or "").replace(" ", "")
        return f"Z{num}{roman}".upper()

    # D ç³»åˆ—ï¼ˆD850ã€D780 ç­‰ï¼‰
    m = re.search(r"\bD[0-9]{3,4}\b", s0, re.I)
    if m:
        return m.group(0).upper()

    return "UNKNOWN"

def guess_model_name_from_filename(filename: str) -> str:
    base = os.path.basename(filename)
    return detect_model_from_string(base)

def guess_model_name_from_text(pages_text: list) -> str:
    # ä»å‰å‡ é¡µæ–‡æœ¬é‡Œæ‰¾å‹å·ï¼ˆå°¼åº·æ‰‹å†Œå¼€å¤´é€šå¸¸æœ‰å‹å·ï¼Œä¾‹å¦‚ "Z f"ï¼‰
    head = " ".join(pages_text[:5])  # å‰5é¡µæ‹¼ä¸€æ®µ
    return detect_model_from_string(head)

def read_with_pdfplumber(pdf_path: str):
    import pdfplumber
    pages = []
    with pdfplumber.open(pdf_path) as pdf:
        for i, p in enumerate(pdf.pages, start=1):
            try:
                txt = p.extract_text() or ""
            except Exception:
                txt = ""
            pages.append((i, txt.strip()))
    return pages

def read_with_pymupdf(pdf_path: str):
    import fitz
    pages = []
    with fitz.open(pdf_path) as doc:
        for i, page in enumerate(doc, start=1):
            try:
                txt = page.get_text("text") or ""
            except Exception:
                txt = ""
            pages.append((i, txt.strip()))
    return pages

def parse_pdf(pdf_path: str, out_path: str):
    # å…ˆè¯»ä¸€é
    pages = read_with_pdfplumber(pdf_path)
    empty_ratio = 0 if not pages else sum(1 for _,t in pages if not t)/len(pages)
    if empty_ratio > 0.6:
        pages = read_with_pymupdf(pdf_path)

    # å…ˆç”¨æ–‡ä»¶åçŒœ
    model = guess_model_name_from_filename(pdf_path)

    # å†ç”¨æ–‡æœ¬å…œåº•ï¼ˆæ–‡ä»¶åæ²¡è¯†åˆ«å‡ºæ¥æ—¶ï¼‰
    if model == "UNKNOWN":
        model = guess_model_name_from_text([t for _, t in pages])

    doc_type = "specs" if "spec" in pdf_path.lower() else "manual"

    kept = 0
    with open(out_path, "w", encoding="utf-8") as f:
        for page_no, text in tqdm(pages, desc=f"Writing {os.path.basename(pdf_path)}"):
            if not text:
                continue
            rec = {
                "model": model,
                "doc_type": doc_type,
                "page": page_no,
                "text": text
            }
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            kept += 1
    print(f"âœ… Parsed {pdf_path} â†’ {out_path}  (model={model}, pages_with_text={kept})")

def batch_parse_all():
    pdf_files = glob.glob(os.path.join(RAW_DIR, "**", "*.pdf"), recursive=True)
    if not pdf_files:
        print(f"âš ï¸ æ²¡æ‰¾åˆ° PDFï¼š{RAW_DIR}/**/*.pdf")
        return
    print(f"ğŸ” Found {len(pdf_files)} PDFs")
    for pdf_path in pdf_files:
        out_name = os.path.basename(pdf_path).replace(".pdf", ".jsonl")
        out_path = os.path.join(PARSED_DIR, out_name)
        parse_pdf(pdf_path, out_path)

if __name__ == "__main__":
    batch_parse_all()
